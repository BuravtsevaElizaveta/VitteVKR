# components/yolo_detection.py
"""
–í–∫–ª–∞–¥–∫–∞ ¬´–î–µ—Ç–µ–∫—Ü–∏—è (YOLO)¬ª
‚Äî –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –¥–µ—Ç–µ–∫—Ü–∏—è + –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è.
‚Äî –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω): –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ¬´—Å—Ç—Ä–∏–º¬ª –≤–æ –≤–∫–ª–∞–¥–∫–µ.
‚Äî –ó–∞–ø–∏—Å—å –≤ detections.db.

"""

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import time, json, uuid, tempfile
import numpy as np
import streamlit as st
import cv2

from utils.db import insert_detection

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ultralytics
try:
    from ultralytics import YOLO
    UL_OK = True
except Exception:
    UL_OK = False


@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ YOLO-–≤–µ—Å–æ–≤‚Ä¶")
def _load_yolo(weights_path: str):
    if not UL_OK:
        raise RuntimeError(
            "–ü–∞–∫–µ—Ç ultralytics –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ultralytics"
        )
    return YOLO(weights_path)


def _annotate(img_bgr: np.ndarray, result, names: Dict[int, str], conf_thr: float, thickness: int) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    """–†–∏—Å—É–µ–º –±–æ–∫—Å—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º (label, conf) –¥–ª—è –ë–î."""
    out = img_bgr.copy()
    detected: List[Tuple[str, float]] = []

    boxes = getattr(result, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return out, detected

    xyxy = boxes.xyxy.cpu().numpy().astype(int)
    conf = boxes.conf.cpu().numpy()
    cls  = boxes.cls.cpu().numpy().astype(int)

    for (x1,y1,x2,y2), c, cl in zip(xyxy, conf, cls):
        if float(c) < conf_thr:
            continue
        label = names.get(int(cl), str(cl))
        detected.append((label, float(c)))
        cv2.rectangle(out, (x1,y1), (x2,y2), (0,255,0), thickness)
        txt = f"{label} {float(c):.2f}"
        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw + 2, y1), (0,255,0), -1)
        cv2.putText(out, txt, (x1+1, y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)

    return out, detected


def _save_summary_to_db(db_path: Path, filename: str, source: str, model_name: str, dets: List[Tuple[str, float]]):
    """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ detections.db."""
    if not dets:
        return
    labels = {}
    for lbl, cf in dets:
        labels.setdefault(lbl, []).append(cf)
    avg = sorted([(lbl, float(np.mean(cfs))) for lbl, cfs in labels.items()],
                 key=lambda x: x[1], reverse=True)
    top5 = avg[:5]
    insert_detection(
        db_path=db_path,
        ts=time.strftime("%Y-%m-%dT%H:%M:%S"),
        kind="yolo",
        source=source,
        filename=filename,
        model=model_name,
        label_en=top5[0][0] if top5 else None,
        label_ru=top5[0][0] if top5 else None,
        score=float(top5[0][1]) if top5 else None,
        json_top5=json.dumps([{"label": l, "score": s} for l, s in top5], ensure_ascii=False),
        price_min=None, price_max=None, currency=None,
        image_path=None,
        plate_number=None, region=None, year_issued=None,
        brand=None, car_type=None, color=None, conf_avg=None,
        extra_json=None
    )


def render_yolo_detection(db_path: Path):
    st.header("üîß –î–µ—Ç–µ–∫—Ü–∏—è (YOLO)")

    with st.sidebar:
        st.subheader("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ YOLO")
        model_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å YOLO", ["YOLOv8"], index=0, key="yolo:ver")
        weights_path = st.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ YOLOv8", "models/YOLOv8.pt", key="yolo:weights")

        st.subheader("–û–ø—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞")
        device = st.radio("–í—ã–±–µ—Ä–∏—Ç–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["CPU", "GPU"], horizontal=True, key="yolo:device")
        device = "cuda" if device == "GPU" else "cpu"

        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏")
        conf_thr = st.slider("–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", 0.0, 1.0, 0.25, 0.01, key="yolo:conf")
        thickness = st.slider("–¢–æ–ª—â–∏–Ω–∞ –ª–∏–Ω–∏–∏", 1, 10, 2, 1, key="yolo:th")

    tabs = st.tabs(["üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üéûÔ∏è –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)"])

    # ‚îÄ‚îÄ‚îÄ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚îÄ‚îÄ‚îÄ
    with tabs[0]:
        img_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="yolo:img_upl")
        if img_file is not None:
            img_bytes = img_file.read()
            np_img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)
            w_view = 520
            st.image(cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB),
                     caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–º–µ–Ω—å—à–µ–Ω–æ)",
                     width=w_view)

            run = st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", key="yolo:img_run")
            if run:
                try:
                    model = _load_yolo(weights_path)
                except Exception as e:
                    st.error(str(e))
                    return

                res = model.predict(np_img, verbose=False, device=device, conf=conf_thr, iou=0.5)
                all_dets: List[Tuple[str, float]] = []
                out = np_img.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    all_dets.extend(dets)

                st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                         caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏",
                         width=w_view)

                _save_summary_to_db(db_path, img_file.name, "image", model_choice, all_dets)
                st.success("–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")

    # ‚îÄ‚îÄ‚îÄ –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω) ‚îÄ‚îÄ‚îÄ
    with tabs[1]:
        vid_file = st.file_uploader("–í–∏–¥–µ–æ (MP4/AVI/MOV)", type=["mp4", "avi", "mov", "mkv"], key="yolo:vid_upl")
        col1, col2 = st.columns(2)
        with col1:
            step = st.slider("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä", 1, 10, 2, 1, key="yolo:vid_step")
        with col2:
            max_frames = st.number_input("–ú–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ (0 ‚Äî –≤—Å–µ)", 0, 50000, 0, 1, key="yolo:vid_max")

        start = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ", type="primary", key="yolo:vid_run")

        if vid_file is not None:
            st.video(vid_file, format="video/mp4")  # –æ—Ä–∏–≥–∏–Ω–∞–ª ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–ª–µ–µ—Ä

        if start and vid_file is not None:
            # –ë–µ–∑ st.secrets: —Å–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—É—Ç—å –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º tmp
            tmp_dir = Path(tempfile.gettempdir())
            tmp_path = tmp_dir / f"_uploaded_{uuid.uuid4().hex}.mp4"
            tmp_path.write_bytes(vid_file.getvalue())

            try:
                model = _load_yolo(weights_path)
            except Exception as e:
                st.error(str(e))
                # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            cap = cv2.VideoCapture(str(tmp_path))
            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            frame_box = st.empty()
            progress = st.progress(0)
            info = st.empty()

            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
            processed = 0
            dets_all: List[Tuple[str, float]] = []

            i = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                i += 1
                if (i - 1) % max(step, 1) != 0:
                    continue

                res = model.predict(frame, verbose=False, device=device, conf=conf_thr, iou=0.5)
                out = frame.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    dets_all.extend(dets)

                frame_box.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                                caption="–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫",
                                width=640)

                processed += 1
                if total > 0:
                    progress.progress(min(100, int(i / total * 100)))
                info.caption(f"–ö–∞–¥—Ä {i}/{total or '‚Ä¶'} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed})")

                if max_frames and processed >= max_frames:
                    break

            cap.release()
            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–¥–ª—è Py3.9 –±–µ–∑ missing_ok)
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except Exception:
                pass

            progress.empty()
            info.empty()

            _save_summary_to_db(db_path, vid_file.name, "video", model_choice, dets_all)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î.")
