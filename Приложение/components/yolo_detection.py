# components/yolo_detection.py
"""
–í–∫–ª–∞–¥–∫–∞ ¬´–î–µ—Ç–µ–∫—Ü–∏—è (YOLO)¬ª
‚Äî –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –¥–µ—Ç–µ–∫—Ü–∏—è + –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è.
‚Äî –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω): –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ¬´—Å—Ç—Ä–∏–º¬ª –≤–æ –≤–∫–ª–∞–¥–∫–µ.
‚Äî –ó–∞–ø–∏—Å—å –≤ detections.db.
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import time, json, uuid, tempfile

import numpy as np
import streamlit as st

# ‚îÄ‚îÄ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç OpenCV
CV2_OK = True
CV2_ERR: Optional[Exception] = None
try:
    import cv2  # type: ignore
except Exception as e:
    CV2_OK = False
    CV2_ERR = e

# ‚îÄ‚îÄ –ë–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç Ultralytics YOLO
ULTRA_OK = True
ULTRA_ERR: Optional[Exception] = None
try:
    if CV2_OK:
        from ultralytics import YOLO  # type: ignore
    else:
        ULTRA_OK = False
        ULTRA_ERR = RuntimeError("Ultralytics –ø—Ä–æ–ø—É—â–µ–Ω, —Ç.–∫. OpenCV –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è")
except Exception as e:
    ULTRA_OK = False
    ULTRA_ERR = e

from utils.db import insert_detection

# ‚îÄ‚îÄ –ó–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
DEFAULT_DB_PATH = Path("detections.db")
DEFAULT_WEIGHTS = "models/YOLOv8.pt"
DEFAULT_DEVICE = "cpu"       # –Ω–∞ Streamlit Cloud –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU
DEFAULT_CONF_THR = 0.35
DEFAULT_IOU_THR = 0.50
DEFAULT_THICKNESS = 2
DEFAULT_MODEL_NAME = "yolov8"

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ YOLO-–≤–µ—Å–æ–≤‚Ä¶")
def _load_yolo(weights_path: str):
    if not ULTRA_OK:
        raise RuntimeError(
            f"–ü–∞–∫–µ—Ç ultralytics –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {ULTRA_ERR!r}. "
            "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ/–∏—Å–ø—Ä–∞–≤—å—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∏–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ –ª–æ–∫–∞–ª—å–Ω–æ."
        )
    return YOLO(weights_path)  # type: ignore[name-defined]


def _annotate(
    img_bgr: np.ndarray,
    result,
    names: Dict[int, str],
    conf_thr: float,
    thickness: int
) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    """–†–∏—Å—É–µ–º –±–æ–∫—Å—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º (label, conf) –¥–ª—è –ë–î."""
    out = img_bgr.copy()
    detected: List[Tuple[str, float]] = []

    boxes = getattr(result, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return out, detected

    xyxy = boxes.xyxy.cpu().numpy().astype(int)
    conf = boxes.conf.cpu().numpy()
    cls  = boxes.cls.cpu().numpy().astype(int)

    for (x1, y1, x2, y2), c, cl in zip(xyxy, conf, cls):
        if float(c) < conf_thr:
            continue
        label = names.get(int(cl), str(cl))
        detected.append((label, float(c)))

        # –µ—Å–ª–∏ cv2 –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç ‚Äî —Å—é–¥–∞ –Ω–µ –ø–æ–ø–∞–¥—ë–º (—Å–º. –ø—Ä–æ–≤–µ—Ä–∫—É CV2_OK)
        cv2.rectangle(out, (x1, y1), (x2, y2), (0, 255, 0), thickness)
        txt = f"{label} {float(c):.2f}"
        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw + 2, y1), (0, 255, 0), -1)
        cv2.putText(out, txt, (x1 + 1, y1 - 4),
                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

    return out, detected


def _save_summary_to_db(
    db_path: Path,
    filename: str,
    source: str,
    model_name: str,
    dets: List[Tuple[str, float]]
):
    """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ detections.db."""
    if not dets:
        return
    labels: Dict[str, List[float]] = {}
    for lbl, cf in dets:
        labels.setdefault(lbl, []).append(cf)
    avg = sorted(
        [(lbl, float(np.mean(cfs))) for lbl, cfs in labels.items()],
        key=lambda x: x[1],
        reverse=True
    )
    top5 = avg[:5]
    insert_detection(
        db_path=db_path,
        ts=time.strftime("%Y-%m-%dT%H:%M:%S"),
        kind="yolo",
        source=source,
        filename=filename,
        model=model_name,
        label_en=top5[0][0] if top5 else None,
        label_ru=top5[0][0] if top5 else None,
        score=float(top5[0][1]) if top5 else None,
        json_top5=json.dumps([{"label": l, "score": s} for l, s in top5], ensure_ascii=False),
        price_min=None, price_max=None, currency=None,
        image_path=None,
        plate_number=None, region=None, year_issued=None,
        brand=None, car_type=None, color=None, conf_avg=None,
        extra_json=None
    )

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def render_yolo_detection():
    st.header("üîß –î–µ—Ç–µ–∫—Ü–∏—è (YOLO)")

    # –ï—Å–ª–∏ OpenCV/Ultralytics –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã ‚Äî –º—è–≥–∫–æ –≤—ã—Ö–æ–¥–∏–º
    if not CV2_OK:
        st.warning(
            "–ú–æ–¥—É–ª—å OpenCV (cv2) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —ç—Ç–æ–º –¥–µ–ø–ª–æ–µ. "
            "–î–µ—Ç–µ–∫—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–∞, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç."
        )
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(CV2_ERR))
        return
    if not ULTRA_OK:
        st.warning("Ultralytics/YOLO –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(ULTRA_ERR))
        return

    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã (UI)
    with st.sidebar:
        st.subheader("YOLO –ø–∞—Ä–∞–º–µ—Ç—Ä—ã")
        weights_path = st.text_input("–ü—É—Ç—å –∫ –≤–µ—Å–∞–º YOLOv8", value=DEFAULT_WEIGHTS)
        device = st.selectbox("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["cpu"], index=0)  # GPU –Ω–∞ Cloud –æ–±—ã—á–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        conf_thr = st.slider("Confidence threshold", 0.05, 0.95, DEFAULT_CONF_THR, 0.05)
        iou_thr  = st.slider("IoU threshold", 0.10, 0.90, DEFAULT_IOU_THR, 0.05)
        thickness = st.slider("–¢–æ–ª—â–∏–Ω–∞ —Ä–∞–º–∫–∏", 1, 6, DEFAULT_THICKNESS, 1)
        db_path_str = st.text_input("DB path (detections.db)", value=str(DEFAULT_DB_PATH))
        model_choice = st.text_input("–ú–µ—Ç–∫–∞ –º–æ–¥–µ–ª–∏ –≤ –ë–î", value=DEFAULT_MODEL_NAME)

    db_path = Path(db_path_str) if db_path_str else DEFAULT_DB_PATH

    tabs = st.tabs(["üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üéûÔ∏è –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)"])

    # ‚îÄ‚îÄ‚îÄ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚îÄ‚îÄ‚îÄ
    with tabs[0]:
        img_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="yolo:img_upl")
        if img_file is not None:
            img_bytes = img_file.read()
            np_img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)
            if np_img is None:
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ.")
                return

            w_view = 520
            st.image(cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB),
                     caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–º–µ–Ω—å—à–µ–Ω–æ)",
                     width=w_view)

            run = st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", key="yolo:img_run")
            if run:
                try:
                    model = _load_yolo(weights_path)
                except Exception as e:
                    st.error(str(e))
                    return

                # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                res = model.predict(np_img, verbose=False, device=device, conf=conf_thr, iou=iou_thr)
                all_dets: List[Tuple[str, float]] = []
                out = np_img.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    all_dets.extend(dets)

                st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                         caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏",
                         width=w_view)

                _save_summary_to_db(db_path, img_file.name, "image", model_choice, all_dets)
                st.success("–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")

    # ‚îÄ‚îÄ‚îÄ –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω) ‚îÄ‚îÄ‚îÄ
    with tabs[1]:
        vid_file = st.file_uploader("–í–∏–¥–µ–æ (MP4/AVI/MOV/MKV)", type=["mp4", "avi", "mov", "mkv"], key="yolo:vid_upl")
        col1, col2 = st.columns(2)
        with col1:
            step = st.slider("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä", 1, 10, 2, 1, key="yolo:vid_step")
        with col2:
            max_frames = st.number_input("–ú–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ (0 ‚Äî –≤—Å–µ)", 0, 50000, 0, 1, key="yolo:vid_max")

        start = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ", type="primary", key="yolo:vid_run")

        if vid_file is not None:
            st.video(vid_file, format="video/mp4")  # –ø—Ä–æ—Å—Ç–æ –ø–ª–µ–µ—Ä –∏—Å—Ö–æ–¥–Ω–∏–∫–∞

        if start and vid_file is not None:
            # –í—Ä–µ–º–µ–Ω–Ω—ã–π –ø—É—Ç—å –¥–ª—è OpenCV
            tmp_dir = Path(tempfile.gettempdir())
            tmp_path = tmp_dir / f"_uploaded_{uuid.uuid4().hex}.mp4"
            tmp_path.write_bytes(vid_file.getvalue())

            try:
                model = _load_yolo(weights_path)
            except Exception as e:
                st.error(str(e))
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            cap = cv2.VideoCapture(str(tmp_path))
            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            frame_box = st.empty()
            progress = st.progress(0)
            info = st.empty()

            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
            processed = 0
            dets_all: List[Tuple[str, float]] = []

            i = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                i += 1
                if (i - 1) % max(step, 1) != 0:
                    continue

                res = model.predict(frame, verbose=False, device=device, conf=conf_thr, iou=iou_thr)
                out = frame.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    dets_all.extend(dets)

                frame_box.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                                caption="–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫",
                                width=640)

                processed += 1
                if total > 0:
                    progress.progress(min(100, int(i / total * 100)))
                info.caption(f"–ö–∞–¥—Ä {i}/{total or '‚Ä¶'} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed})")

                if max_frames and processed >= max_frames:
                    break

            cap.release()
            # –ß–∏—Å—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except Exception:
                pass

            progress.empty()
            info.empty()

            _save_summary_to_db(db_path, vid_file.name, "video", model_choice, dets_all)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î.")
