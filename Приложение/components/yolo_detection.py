# components/yolo_detection.py
"""
–í–∫–ª–∞–¥–∫–∞ ¬´–î–µ—Ç–µ–∫—Ü–∏—è (YOLO)¬ª ‚Äî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ/–≤–∏–¥–µ–æ —Å –∞–Ω–Ω–æ—Ç–∞—Ü–∏–µ–π –∏ –∑–∞–ø–∏—Å—å—é –≤ –ë–î.
"""
from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import time, json, uuid, tempfile
import numpy as np
import streamlit as st

# --- –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π utils —Ä–∞–Ω—å—à–µ –ª—é–±—ã—Ö –≤–Ω–µ—à–Ω–∏—Ö utils ---
import sys
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

# –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç OpenCV
CV2_OK = True
CV2_ERR: Optional[Exception] = None
try:
    import cv2  # type: ignore
except Exception as e:
    CV2_OK = False
    CV2_ERR = e

# –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç ultralytics
UL_OK = True
UL_ERR: Optional[Exception] = None
try:
    if CV2_OK:
        from ultralytics import YOLO  # type: ignore
    else:
        UL_OK = False
        UL_ERR = RuntimeError("Ultralytics –ø—Ä–æ–ø—É—â–µ–Ω, —Ç.–∫. OpenCV –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è")
except Exception as e:
    UL_OK = False
    UL_ERR = e

from utils.db import insert_detection

@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ YOLO-–≤–µ—Å–æ–≤‚Ä¶")
def _load_yolo(weights_path: str):
    if not UL_OK:
        raise RuntimeError("–ü–∞–∫–µ—Ç ultralytics –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —ç—Ç–æ–º –¥–µ–ø–ª–æ–µ.")
    return YOLO(weights_path)

def _annotate(img_bgr: np.ndarray, result, names: Dict[int, str], conf_thr: float, thickness: int) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    out = img_bgr.copy()
    detected: List[Tuple[str, float]] = []
    boxes = getattr(result, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return out, detected
    xyxy = boxes.xyxy.cpu().numpy().astype(int)
    conf = boxes.conf.cpu().numpy()
    cls  = boxes.cls.cpu().numpy().astype(int)
    for (x1,y1,x2,y2), c, cl in zip(xyxy, conf, cls):
        if float(c) < conf_thr: continue
        label = names.get(int(cl), str(cl))
        detected.append((label, float(c)))
        cv2.rectangle(out, (x1,y1), (x2,y2), (0,255,0), thickness)
        txt = f"{label} {float(c):.2f}"
        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw + 2, y1), (0,255,0), -1)
        cv2.putText(out, txt, (x1+1, y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)
    return out, detected

def _save_summary_to_db(db_path: Path, filename: str, source: str, model_name: str, dets: List[Tuple[str, float]]):
    if not dets: return
    labels: Dict[str, list] = {}
    for lbl, cf in dets:
        labels.setdefault(lbl, []).append(cf)
    avg = sorted([(lbl, float(np.mean(cfs))) for lbl, cfs in labels.items()], key=lambda x: x[1], reverse=True)
    top5 = avg[:5]
    insert_detection(
        db_path=db_path,
        ts=time.strftime("%Y-%m-%dT%H:%M:%S"),
        kind="yolo", source=source, filename=filename, model=model_name,
        label_en=top5[0][0] if top5 else None, label_ru=top5[0][0] if top5 else None,
        score=float(top5[0][1]) if top5 else None,
        json_top5=json.dumps([{"label": l, "score": s} for l, s in top5], ensure_ascii=False),
        price_min=None, price_max=None, currency=None, image_path=None,
        plate_number=None, region=None, year_issued=None,
        brand=None, car_type=None, color=None, conf_avg=None, extra_json=None
    )

def render_yolo_detection(db_path: Path):
    st.header("üîß –î–µ—Ç–µ–∫—Ü–∏—è (YOLO)")
    if not CV2_OK:
        st.warning("OpenCV (cv2) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        if CV2_ERR:
            with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–∞–ª—å"):
                st.code(repr(CV2_ERR))
        return
    if not UL_OK:
        st.warning("Ultralytics/YOLO –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        if UL_ERR:
            with st.expander("–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –¥–µ—Ç–∞–ª—å"):
                st.code(repr(UL_ERR))
        return

    with st.sidebar:
        st.subheader("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ YOLO")
        model_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å YOLO", ["YOLOv8"], index=0, key="yolo:ver")
        weights_path = st.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ YOLOv8", "models/YOLOv8.pt", key="yolo:weights")
        st.subheader("–û–ø—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞")
        device = st.radio("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["CPU", "GPU"], horizontal=True, key="yolo:device")
        device = "cuda" if device == "GPU" else "cpu"
        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏")
        conf_thr = st.slider("–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", 0.0, 1.0, 0.25, 0.01, key="yolo:conf")
        thickness = st.slider("–¢–æ–ª—â–∏–Ω–∞ –ª–∏–Ω–∏–∏", 1, 10, 2, 1, key="yolo:th")

    tabs = st.tabs(["üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üéûÔ∏è –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)"])

    # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    with tabs[0]:
        img_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="yolo:img_upl")
        if img_file is not None:
            img_bytes = img_file.read()
            np_img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)
            w_view = 520
            st.image(cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB), caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–º–µ–Ω—å—à–µ–Ω–æ)", width=w_view)
            run = st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", key="yolo:img_run")
            if run:
                try:
                    model = _load_yolo(weights_path)
                except Exception as e:
                    st.error(str(e)); return
                res = model.predict(np_img, verbose=False, device=device, conf=conf_thr, iou=0.5)
                all_dets: List[Tuple[str, float]] = []
                out = np_img.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    all_dets.extend(dets)
                st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB), caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏", width=w_view)
                _save_summary_to_db(db_path, img_file.name, "image", model_choice, all_dets)
                st.success("–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")

    # –í–∏–¥–µ–æ
    with tabs[1]:
        vid_file = st.file_uploader("–í–∏–¥–µ–æ (MP4/AVI/MOV)", type=["mp4", "avi", "mov", "mkv"], key="yolo:vid_upl")
        col1, col2 = st.columns(2)
        with col1:
            step = st.slider("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä", 1, 10, 2, 1, key="yolo:vid_step")
        with col2:
            max_frames = st.number_input("–ú–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ (0 ‚Äî –≤—Å–µ)", 0, 50000, 0, 1, key="yolo:vid_max")
        start = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ", type="primary", key="yolo:vid_run")
        if vid_file is not None:
            st.video(vid_file, format="video/mp4")
        if start and vid_file is not None:
            tmp_dir = Path(tempfile.gettempdir())
            tmp_path = tmp_dir / f"_uploaded_{uuid.uuid4().hex}.mp4"
            tmp_path.write_bytes(vid_file.getvalue())
            try:
                model = _load_yolo(weights_path)
            except Exception as e:
                st.error(str(e))
                try:
                    if tmp_path.exists(): tmp_path.unlink()
                except Exception:
                    pass
                return
            cap = cv2.VideoCapture(str(tmp_path))
            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
                try:
                    if tmp_path.exists(): tmp_path.unlink()
                except Exception:
                    pass
                return
            frame_box = st.empty()
            progress = st.progress(0)
            info = st.empty()
            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
            processed = 0
            dets_all: List[Tuple[str, float]] = []
            i = 0
            while True:
                ret, frame = cap.read()
                if not ret: break
                i += 1
                if (i - 1) % max(step, 1) != 0: continue
                res = model.predict(frame, verbose=False, device=device, conf=conf_thr, iou=0.5)
                out = frame.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    dets_all.extend(dets)
                frame_box.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB), caption="–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫", width=640)
                processed += 1
                if total > 0:
                    progress.progress(min(100, int(i / total * 100)))
                info.caption(f"–ö–∞–¥—Ä {i}/{total or '‚Ä¶'} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed})")
                if max_frames and processed >= max_frames: break
            cap.release()
            try:
                if tmp_path.exists(): tmp_path.unlink()
            except Exception:
                pass
            progress.empty(); info.empty()
            _save_summary_to_db(db_path, vid_file.name, "video", model_choice, dets_all)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î.")
