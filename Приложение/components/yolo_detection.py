# components/yolo_detection.py
"""
–í–∫–ª–∞–¥–∫–∞ ¬´–î–µ—Ç–µ–∫—Ü–∏—è (YOLO)¬ª ‚Äî –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ—Ñ–ª–∞–π–Ω-–≤–∏–¥–µ–æ.
"""

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import time, json, uuid, tempfile
import numpy as np
import streamlit as st

# ‚îÄ‚îÄ –±–µ–∑–æ–ø–∞—Å–Ω—ã–π –∏–º–ø–æ—Ä—Ç cv2 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CV2_OK, CV2_ERR = True, None
try:
    import cv2  # –¥–æ–ª–∂–µ–Ω –ø—Ä–∏–π—Ç–∏ –∏–∑ opencv-python-headless
except Exception as e:
    CV2_OK, CV2_ERR = False, e

# ‚îÄ‚îÄ ultralytics —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ cv2 –ø–æ–¥–Ω—è–ª—Å—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
UL_OK, UL_ERR = True, None
try:
    if CV2_OK:
        from ultralytics import YOLO
    else:
        UL_OK, UL_ERR = False, RuntimeError("Ultralytics –ø—Ä–æ–ø—É—â–µ–Ω: OpenCV –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è")
except Exception as e:
    UL_OK, UL_ERR = False, e

from utils.db import insert_detection


@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ YOLO-–≤–µ—Å–æ–≤‚Ä¶")
def _load_yolo(weights_path: str):
    if not UL_OK:
        raise RuntimeError(
            "–ü–∞–∫–µ—Ç ultralytics –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ requirements.txt –ù–ï–¢ opencv-python, "
            "–∞ –µ—Å—Ç—å opencv-python-headless."
        )
    return YOLO(weights_path)


def _annotate(img_bgr: np.ndarray, result, names: Dict[int, str],
              conf_thr: float, thickness: int) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    out = img_bgr.copy()
    detected: List[Tuple[str, float]] = []
    boxes = getattr(result, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return out, detected
    xyxy = boxes.xyxy.cpu().numpy().astype(int)
    conf = boxes.conf.cpu().numpy()
    cls  = boxes.cls.cpu().numpy().astype(int)
    for (x1,y1,x2,y2), c, cl in zip(xyxy, conf, cls):
        if float(c) < conf_thr:
            continue
        label = names.get(int(cl), str(cl))
        detected.append((label, float(c)))
        cv2.rectangle(out, (x1,y1), (x2,y2), (0,255,0), thickness)
        txt = f"{label} {float(c):.2f}"
        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw + 2, y1), (0,255,0), -1)
        cv2.putText(out, txt, (x1+1, y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)
    return out, detected


def _save_summary_to_db(db_path: Path, filename: str, source: str,
                        model_name: str, dets: List[Tuple[str, float]]):
    if not dets:
        return
    labels: Dict[str, List[float]] = {}
    for lbl, cf in dets:
        labels.setdefault(lbl, []).append(cf)
    avg = sorted([(lbl, float(np.mean(cfs))) for lbl, cfs in labels.items()],
                 key=lambda x: x[1], reverse=True)
    top5 = avg[:5]
    insert_detection(
        db_path=db_path,
        ts=time.strftime("%Y-%m-%dT%H:%M:%S"),
        kind="yolo",
        source=source,
        filename=filename,
        model=model_name,
        label_en=top5[0][0] if top5 else None,
        label_ru=top5[0][0] if top5 else None,
        score=float(top5[0][1]) if top5 else None,
        json_top5=json.dumps([{"label": l, "score": s} for l, s in top5], ensure_ascii=False),
        price_min=None, price_max=None, currency=None,
        image_path=None,
        plate_number=None, region=None, year_issued=None,
        brand=None, car_type=None, color=None, conf_avg=None,
        extra_json=None
    )


def render_yolo_detection(db_path: Path):
    st.header("üîß –î–µ—Ç–µ–∫—Ü–∏—è (YOLO)")

    # –ï—Å–ª–∏ OpenCV –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è ‚Äî –æ–±—ä—è—Å–Ω—è–µ–º –ø—Ä–∏—á–∏–Ω—É –∏ –≤—ã—Ö–æ–¥–∏–º
    if not CV2_OK:
        st.warning("OpenCV (cv2) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(CV2_ERR))
        st.info("–û–±—ã—á–Ω–æ —ç—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç, –∫–æ–≥–¥–∞ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–∞–∫–µ—Ç opencv-python –≤–º–µ—Å—Ç–æ opencv-python-headless.")
        return
    if not UL_OK:
        st.warning("Ultralytics YOLO –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω.")
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(UL_ERR))
        return

    with st.sidebar:
        st.subheader("–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏ YOLO")
        model_choice = st.selectbox("–í—ã–±–µ—Ä–∏—Ç–µ –º–æ–¥–µ–ª—å YOLO", ["YOLOv8"], index=0, key="yolo:ver")
        weights_path = st.text_input("–ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ YOLOv8", "–ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ/models/YOLOv8.pt", key="yolo:weights")

        st.subheader("–û–ø—Ü–∏–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞")
        device = st.radio("–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ", ["CPU", "GPU"], horizontal=True, key="yolo:device")
        device = "cuda" if device == "GPU" else "cpu"

        st.subheader("–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–µ—Ç–µ–∫—Ü–∏–∏")
        conf_thr = st.slider("–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏", 0.0, 1.0, 0.25, 0.01, key="yolo:conf")
        thickness = st.slider("–¢–æ–ª—â–∏–Ω–∞ –ª–∏–Ω–∏–∏", 1, 10, 2, 1, key="yolo:th")

    tabs = st.tabs(["üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üéûÔ∏è –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)"])

    # ‚îÄ‚îÄ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    with tabs[0]:
        img_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="yolo:img_upl")
        if img_file is not None:
            np_img = cv2.imdecode(np.frombuffer(img_file.read(), np.uint8), cv2.IMREAD_COLOR)
            st.image(cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB),
                     caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–º–µ–Ω—å—à–µ–Ω–æ)", width=520)

            if st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", key="yolo:img_run"):
                try:
                    model = _load_yolo(weights_path)
                except Exception as e:
                    st.error(str(e)); return

                res = model.predict(np_img, verbose=False, device=device, conf=conf_thr, iou=0.5)
                all_dets: List[Tuple[str, float]] = []
                out = np_img.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    all_dets.extend(dets)

                st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB), caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏", width=520)
                _save_summary_to_db(db_path, img_file.name, "image", model_choice, all_dets)
                st.success("–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")

    # ‚îÄ‚îÄ –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)
    with tabs[1]:
        vid_file = st.file_uploader("–í–∏–¥–µ–æ (MP4/AVI/MOV/MKV)", type=["mp4", "avi", "mov", "mkv"], key="yolo:vid_upl")
        col1, col2 = st.columns(2)
        with col1:
            step = st.slider("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä", 1, 10, 2, 1, key="yolo:vid_step")
        with col2:
            max_frames = st.number_input("–ú–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ (0 ‚Äî –≤—Å–µ)", 0, 50000, 0, 1, key="yolo:vid_max")

        start = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ", type="primary", key="yolo:vid_run")

        if vid_file is not None:
            st.video(vid_file, format="video/mp4")  # –ø—Ä–æ—Å—Ç–æ –ø–ª–µ–µ—Ä

        if start and vid_file is not None:
            tmp = Path(tempfile.gettempdir()) / f"_uploaded_{uuid.uuid4().hex}.mp4"
            tmp.write_bytes(vid_file.getvalue())
            try:
                model = _load_yolo(weights_path)
            except Exception as e:
                st.error(str(e))
                try:
                    if tmp.exists(): tmp.unlink()
                except Exception:
                    pass
                return

            cap = cv2.VideoCapture(str(tmp))
            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
                try:
                    if tmp.exists(): tmp.unlink()
                except Exception:
                    pass
                return

            frame_box = st.empty()
            progress = st.progress(0)
            info = st.empty()
            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
            processed = 0
            dets_all: List[Tuple[str, float]] = []

            i = 0
            while True:
                ret, frame = cap.read()
                if not ret: break
                i += 1
                if (i - 1) % max(step, 1) != 0:
                    continue

                res = model.predict(frame, verbose=False, device=device, conf=conf_thr, iou=0.5)
                out = frame.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    dets_all.extend(dets)

                frame_box.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB), caption="–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫", width=640)

                processed += 1
                if total > 0:
                    progress.progress(min(100, int(i / total * 100)))
                info.caption(f"–ö–∞–¥—Ä {i}/{total or '‚Ä¶'} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed})")

                if max_frames and processed >= max_frames:
                    break

            cap.release()
            try:
                if tmp.exists(): tmp.unlink()
            except Exception:
                pass

            progress.empty(); info.empty()
            _save_summary_to_db(db_path, vid_file.name, "video", model_choice, dets_all)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î.")

