# components/yolo_detection.py
"""
–í–∫–ª–∞–¥–∫–∞ ¬´–î–µ—Ç–µ–∫—Ü–∏—è (YOLO)¬ª
‚Äî –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: –¥–µ—Ç–µ–∫—Ü–∏—è + –∞–Ω–Ω–æ—Ç–∞—Ü–∏—è.
‚Äî –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω): –∑–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –∞–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π ¬´—Å—Ç—Ä–∏–º¬ª –≤–æ –≤–∫–ª–∞–¥–∫–µ.
‚Äî –ó–∞–ø–∏—Å—å –≤ detections.db.

"""

from __future__ import annotations
from pathlib import Path
from typing import List, Tuple, Optional, Dict
import time, json, uuid, tempfile
import numpy as np
import streamlit as st
import cv2

from utils.db import insert_detection

CV2_OK = True
CV2_ERR = None
ULTRA_OK = True
ULTRA_ERR = None

try:
    import cv2  # OpenCV
except Exception as e:
    CV2_OK = False
    CV2_ERR = e

try:
    # –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ ultralytics/torch —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ cv2 –µ—Å—Ç—å
    if CV2_OK:
        from ultralytics import YOLO
    else:
        ULTRA_OK = False
        ULTRA_ERR = RuntimeError("Ultralytics –ø—Ä–æ–ø—É—â–µ–Ω, —Ç.–∫. OpenCV –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª—Å—è")
except Exception as e:
    ULTRA_OK = False
    ULTRA_ERR = e

@st.cache_resource(show_spinner="–ó–∞–≥—Ä—É–∑–∫–∞ YOLO-–≤–µ—Å–æ–≤‚Ä¶")
def _load_yolo(weights_path: str):
    if not UL_OK:
        raise RuntimeError(
            "–ü–∞–∫–µ—Ç ultralytics –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ultralytics"
        )
    return YOLO(weights_path)


def _annotate(img_bgr: np.ndarray, result, names: Dict[int, str], conf_thr: float, thickness: int) -> Tuple[np.ndarray, List[Tuple[str, float]]]:
    """–†–∏—Å—É–µ–º –±–æ–∫—Å—ã –∏ —Å–æ–±–∏—Ä–∞–µ–º (label, conf) –¥–ª—è –ë–î."""
    out = img_bgr.copy()
    detected: List[Tuple[str, float]] = []

    boxes = getattr(result, "boxes", None)
    if boxes is None or len(boxes) == 0:
        return out, detected

    xyxy = boxes.xyxy.cpu().numpy().astype(int)
    conf = boxes.conf.cpu().numpy()
    cls  = boxes.cls.cpu().numpy().astype(int)

    for (x1,y1,x2,y2), c, cl in zip(xyxy, conf, cls):
        if float(c) < conf_thr:
            continue
        label = names.get(int(cl), str(cl))
        detected.append((label, float(c)))
        cv2.rectangle(out, (x1,y1), (x2,y2), (0,255,0), thickness)
        txt = f"{label} {float(c):.2f}"
        (tw, th), _ = cv2.getTextSize(txt, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)
        cv2.rectangle(out, (x1, y1 - th - 6), (x1 + tw + 2, y1), (0,255,0), -1)
        cv2.putText(out, txt, (x1+1, y1-4), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,0), 1, cv2.LINE_AA)

    return out, detected


def _save_summary_to_db(db_path: Path, filename: str, source: str, model_name: str, dets: List[Tuple[str, float]]):
    """–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–ø–∏—Å—å –æ–¥–Ω–æ–π —Å–µ—Å—Å–∏–∏ –¥–µ—Ç–µ–∫—Ü–∏–π –≤ detections.db."""
    if not dets:
        return
    labels = {}
    for lbl, cf in dets:
        labels.setdefault(lbl, []).append(cf)
    avg = sorted([(lbl, float(np.mean(cfs))) for lbl, cfs in labels.items()],
                 key=lambda x: x[1], reverse=True)
    top5 = avg[:5]
    insert_detection(
        db_path=db_path,
        ts=time.strftime("%Y-%m-%dT%H:%M:%S"),
        kind="yolo",
        source=source,
        filename=filename,
        model=model_name,
        label_en=top5[0][0] if top5 else None,
        label_ru=top5[0][0] if top5 else None,
        score=float(top5[0][1]) if top5 else None,
        json_top5=json.dumps([{"label": l, "score": s} for l, s in top5], ensure_ascii=False),
        price_min=None, price_max=None, currency=None,
        image_path=None,
        plate_number=None, region=None, year_issued=None,
        brand=None, car_type=None, color=None, conf_avg=None,
        extra_json=None
    )


def render_yolo_detection():
    st.header("üîß –î–µ—Ç–µ–∫—Ü–∏—è (YOLO)")
    if not CV2_OK:
        st.warning(
            "–ú–æ–¥—É–ª—å OpenCV (cv2) –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤ —ç—Ç–æ–º –¥–µ–ø–ª–æ–µ. "
            "–î–µ—Ç–µ–∫—Ü–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω–∞, –æ—Å—Ç–∞–ª—å–Ω—ã–µ –≤–∫–ª–∞–¥–∫–∏ —Ä–∞–±–æ—Ç–∞—é—Ç."
        )
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(CV2_ERR))
        return
    if not ULTRA_OK:
        st.warning("YOLO –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω. –î–µ—Ç–µ–∫—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞.")
        with st.expander("–ü–æ–∫–∞–∑–∞—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –¥–µ—Ç–∞–ª—å –æ—à–∏–±–∫–∏"):
            st.code(repr(ULTRA_ERR))
        return

    tabs = st.tabs(["üñºÔ∏è –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", "üéûÔ∏è –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω)"])

    # ‚îÄ‚îÄ‚îÄ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ ‚îÄ‚îÄ‚îÄ
    with tabs[0]:
        img_file = st.file_uploader("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (JPG/PNG)", type=["jpg", "jpeg", "png"], key="yolo:img_upl")
        if img_file is not None:
            img_bytes = img_file.read()
            np_img = cv2.imdecode(np.frombuffer(img_bytes, np.uint8), cv2.IMREAD_COLOR)
            w_view = 520
            st.image(cv2.cvtColor(np_img, cv2.COLOR_BGR2RGB),
                     caption="–ó–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (—É–º–µ–Ω—å—à–µ–Ω–æ)",
                     width=w_view)

            run = st.button("–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ", type="primary", key="yolo:img_run")
            if run:
                try:
                    model = _load_yolo(weights_path)
                except Exception as e:
                    st.error(str(e))
                    return

                res = model.predict(np_img, verbose=False, device=device, conf=conf_thr, iou=0.5)
                all_dets: List[Tuple[str, float]] = []
                out = np_img.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    all_dets.extend(dets)

                st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                         caption="–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–µ—Ç–µ–∫—Ü–∏–∏",
                         width=w_view)

                _save_summary_to_db(db_path, img_file.name, "image", model_choice, all_dets)
                st.success("–ì–æ—Ç–æ–≤–æ. –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ –ë–î.")

    # ‚îÄ‚îÄ‚îÄ –í–∏–¥–µ–æ (–æ—Ñ–ª–∞–π–Ω) ‚îÄ‚îÄ‚îÄ
    with tabs[1]:
        vid_file = st.file_uploader("–í–∏–¥–µ–æ (MP4/AVI/MOV)", type=["mp4", "avi", "mov", "mkv"], key="yolo:vid_upl")
        col1, col2 = st.columns(2)
        with col1:
            step = st.slider("–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∫–∞–∂–¥—ã–π N-–π –∫–∞–¥—Ä", 1, 10, 2, 1, key="yolo:vid_step")
        with col2:
            max_frames = st.number_input("–ú–∞–∫—Å. –∫–∞–¥—Ä–æ–≤ (0 ‚Äî –≤—Å–µ)", 0, 50000, 0, 1, key="yolo:vid_max")

        start = st.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É –≤–∏–¥–µ–æ", type="primary", key="yolo:vid_run")

        if vid_file is not None:
            st.video(vid_file, format="video/mp4")  # –æ—Ä–∏–≥–∏–Ω–∞–ª ‚Äî –ø—Ä–æ—Å—Ç–æ –ø–ª–µ–µ—Ä

        if start and vid_file is not None:
            # –ë–µ–∑ st.secrets: —Å–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø—É—Ç—å –≤ —Å–∏—Å—Ç–µ–º–Ω–æ–º tmp
            tmp_dir = Path(tempfile.gettempdir())
            tmp_path = tmp_dir / f"_uploaded_{uuid.uuid4().hex}.mp4"
            tmp_path.write_bytes(vid_file.getvalue())

            try:
                model = _load_yolo(weights_path)
            except Exception as e:
                st.error(str(e))
                # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            cap = cv2.VideoCapture(str(tmp_path))
            if not cap.isOpened():
                st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ.")
                try:
                    if tmp_path.exists():
                        tmp_path.unlink()
                except Exception:
                    pass
                return

            frame_box = st.empty()
            progress = st.progress(0)
            info = st.empty()

            total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
            processed = 0
            dets_all: List[Tuple[str, float]] = []

            i = 0
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                i += 1
                if (i - 1) % max(step, 1) != 0:
                    continue

                res = model.predict(frame, verbose=False, device=device, conf=conf_thr, iou=0.5)
                out = frame.copy()
                for r in res:
                    names = getattr(r, "names", {}) or {}
                    out, dets = _annotate(out, r, names, conf_thr, thickness)
                    dets_all.extend(dets)

                frame_box.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB),
                                caption="–ê–Ω–Ω–æ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–æ—Ç–æ–∫",
                                width=640)

                processed += 1
                if total > 0:
                    progress.progress(min(100, int(i / total * 100)))
                info.caption(f"–ö–∞–¥—Ä {i}/{total or '‚Ä¶'} (–æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {processed})")

                if max_frames and processed >= max_frames:
                    break

            cap.release()
            # –ß–∏—Å—Ç–∏–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª (–¥–ª—è Py3.9 –±–µ–∑ missing_ok)
            try:
                if tmp_path.exists():
                    tmp_path.unlink()
            except Exception:
                pass

            progress.empty()
            info.empty()

            _save_summary_to_db(db_path, vid_file.name, "video", model_choice, dets_all)
            st.success("–û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–∏–¥–µ–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ë–î.")

