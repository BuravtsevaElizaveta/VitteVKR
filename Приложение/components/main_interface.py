"""
components/main_interface.py
Ğ¡Ñ‚Ñ€Ğ¸Ğ¼Ğ»Ğ¸Ñ‚-Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ° YOLO Ñ Ğ²Ñ‹Ğ²Ğ¾Ğ´Ğ¾Ğ¼ Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚ĞµĞ¹ Ğ¸ Ğ±ĞµĞ·Ğ¾Ğ¿Ğ°ÑĞ½Ğ¾Ğ¹ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¾Ğ¹ Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¾Ğ².
"""

import logging
import tempfile
import json
from typing import Dict, List, Any

import cv2
import numpy as np
import streamlit as st
from PIL import Image
from streamlit_webrtc import webrtc_streamer, VideoTransformerBase

from utils.image_processing import draw_boxes
from utils.video_processing import (
    process_uploaded_video,
    process_video_realtime,
    process_rtsp_stream,
)
from utils.database import log_detections_bulk
from utils.helpers import format_detections
from utils.chat_comment import summarize_detections
from utils.parts_info import get_part_info

logger = logging.getLogger(__name__)


@st.cache_resource(show_spinner="Ğ—Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ° YOLOâ€¦")
def load_model(opt: str, weights: str, device_raw: str):
    device = (device_raw or "cpu").strip().lower()
    if device not in ("cpu", "cuda", "mps"):
        device = "cpu"
    try:
        if opt == "YOLOv9":
            from models.yolo_v9 import YOLOv9
            return YOLOv9(weights, device)
        if opt == "YOLOv8":
            from models.yolo_v8 import YOLOv8
            return YOLOv8(weights, device)
    except ImportError:
        pass
    from ultralytics import YOLO
    model = YOLO(weights)
    if device in ("cuda", "mps"):
        try:
            model.to(device)
        except Exception:
            pass
    return model


class VideoTransformer(VideoTransformerBase):
    def __init__(self, model, conf, thick, db):
        self.model, self.conf, self.thick, self.db = model, conf, thick, db
        self.counts: Dict[str, int] = {}

    def transform(self, frame):
        img = frame.to_ndarray(format="bgr24")
        dets = self.model.predict(img, conf=self.conf)
        img = draw_boxes(img, dets, self.conf, self.model, self.thick)
        for d in dets:
            cls = int(d[5])
            nm = self.model.model.names[cls]
            self.counts[nm] = self.counts.get(nm, 0) + 1

        # Ğ°Ğ²Ñ‚Ğ¾-ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸Ğ¹ ĞºĞ°Ğ¶Ğ´Ñ‹Ğµ 60 Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¹
        if sum(self.counts.values()) % 60 == 0 and self.counts:
            comment = summarize_detections(self.counts)
            st.session_state.setdefault("_live_comment", st.empty()).markdown(
                f"**ğŸ’¬ Ğ˜Ğ˜-Ğ°ÑÑĞ¸ÑÑ‚ĞµĞ½Ñ‚:** {comment}"
            )
            self.counts.clear()

        # Ğ»Ğ¾Ğ³ Ğ² Ğ‘Ğ” Ñ€Ğ°Ğ· Ğ² 30 ĞºĞ°Ğ´Ñ€Ğ¾Ğ²
        if sum(self.counts.values()) % 30 == 0:
            try:
                log_detections_bulk(dets, self.db)
            except Exception:
                pass
        return img


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â«Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸ĞµÂ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_image(model, conf, thick, db):
    img_file = st.file_uploader("Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ (JPG/PNG)", ["jpg", "jpeg", "png"])
    if img_file is None:
        return

    pil = Image.open(img_file)
    img = cv2.cvtColor(np.asarray(pil), cv2.COLOR_RGB2BGR)

    dets = model.predict(img, conf=conf)
    out = draw_boxes(img.copy(), dets, conf, model, thick)
    st.image(cv2.cvtColor(out, cv2.COLOR_BGR2RGB), width=640)

    try:
        log_detections_bulk(dets, db)
    except Exception:
        pass

    # Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ´ĞµÑ‚ĞµĞºÑ†Ğ¸Ğ¸ Ğ¼Ğ¾Ğ³ÑƒÑ‚ Ğ±Ñ‹Ñ‚ÑŒ ÑÑ‚Ñ€Ğ¾ĞºĞ¾Ğ¹ Ğ¸Ğ»Ğ¸ ÑĞ¿Ğ¸ÑĞºĞ¾Ğ¼ ÑĞ»Ğ¾Ğ²Ğ°Ñ€ĞµĞ¹
    formatted_json = format_detections(dets, model)
    st.json(formatted_json)

    if isinstance(formatted_json, str):
        try:
            formatted_list: List[Any] = json.loads(formatted_json)
        except Exception:
            formatted_list = []
    else:
        formatted_list = formatted_json

    rows = []
    for item in formatted_list:
        if not isinstance(item, dict):
            continue
        cls_name = item.get("class") or item.get("label")
        if cls_name is None:
            continue
        info = get_part_info(cls_name)
        rows.append({
            "Ğ—Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚ÑŒ": info["name"],
            "Ğ£Ğ²ĞµÑ€ĞµĞ½Ğ½Ğ¾ÑÑ‚ÑŒ": f"{item.get('confidence', 0):.2f}",
            "ĞĞ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ": info["description"],
        })

    if rows:
        st.subheader("Ğ˜Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¿Ğ¾ Ğ°Ğ²Ñ‚Ğ¾Ğ·Ğ°Ğ¿Ñ‡Ğ°ÑÑ‚ÑĞ¼")
        st.table(rows)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â«Ğ’Ğ¸Ğ´ĞµĞ¾ off-lineÂ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_video_offline(model, conf, thick, db):
    vid = st.file_uploader("Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ„Ğ°Ğ¹Ğ» (offline)", ["mp4", "mov", "avi"])
    if vid is None:
        return
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(vid.read())
        path = tmp.name
    st.info("â³ ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾â€¦")
    out_path = process_uploaded_video(path, model, conf, thick, db)
    st.video(out_path, format="video/mp4", start_time=0)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â«Ğ’Ğ¸Ğ´ĞµĞ¾ on-lineÂ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_video_online(model, conf, thick, db):
    vid = st.file_uploader("Ğ’Ğ¸Ğ´ĞµĞ¾-Ñ„Ğ°Ğ¹Ğ» (online)", ["mp4", "mov", "avi"])
    if vid is None:
        return
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp4") as tmp:
        tmp.write(vid.read()); path = tmp.name
    st_frame = st.empty()
    for frame, _ in process_video_realtime(path, model, conf, thick, db):
        st_frame.image(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), width=640)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Â«RTSP / WebRTCÂ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _render_rtsp(model, conf, thick, db):
    url = st.text_input("RTSP-URL (Ğ¿ÑƒÑÑ‚Ğ¾ â†’ WebRTC)")
    if url:
        process_rtsp_stream(url, model, conf, thick, db)
    else:
        webrtc_streamer(
            key="yolo-webrtc",
            video_transformer_factory=lambda: VideoTransformer(model, conf, thick, db),
            rtc_configuration={"iceServers": [{"urls": []}]},
        )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¸Ğ½Ñ‚ĞµÑ€Ñ„ĞµĞ¹Ñ Ğ²ĞºĞ»Ğ°Ğ´ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def render_detection_interface(opt, weights, device, conf, thick, db):
    if not weights:
        st.warning("Ğ£ĞºĞ°Ğ¶Ğ¸Ñ‚Ğµ Ğ¿ÑƒÑ‚ÑŒ Ğº Ğ²ĞµÑĞ°Ğ¼.")
        return
    model = load_model(opt, weights, device)

    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ–¼ Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ", "ğŸ Ğ’Ğ¸Ğ´ĞµĞ¾ offline", "âš¡ Ğ’Ğ¸Ğ´ĞµĞ¾ online", "ğŸ“¡ Live / RTSP"]
    )
    with tab1:
        _render_image(model, conf, thick, db)
    with tab2:
        _render_video_offline(model, conf, thick, db)
    with tab3:
        _render_video_online(model, conf, thick, db)
    with tab4:
        _render_rtsp(model, conf, thick, db)
